---
title: "HW2"
author: "Jay Westreich"
date: "9/29/23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(stats)
library(markdown)
library(ggplot2)
```

#1a. Using the statisticians design, identify all 11 possible samples given this design. Using the estimator Y=yi/pi, calculate the estimate of the total weight of the elephants for each sample. Make a histogram of these estimates. Report the mean of these 11 estimates.
```{r}
elephant_df <- tibble(
  Elephant = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11),
  Weight = c(9000, 9800, 10000, 10100, 10200, 11000, 11100, 11300, 11700, 12200, 13000),
  Probability = c(0.01, 0.01, 0.01, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01, 0.01, 0.01)
)

samples<-elephant_df%>%
  select(Elephant, Weight)%>%
  rename(sample=Elephant)

print("11 possible samples")
print(samples)
```

```{r}
weight_estimates<-elephant_df%>%
  mutate(estimate=Weight/Probability)

print(weight_estimates)

hist(weight_estimates$estimate)

print(paste0("Mean of 11 estimates: ",mean(weight_estimates$estimate)))

```

#1b. Basu’s point is that the “long-run” properties may not be a good standard for judging estimators. Nevertheless, please examine an alternative sample design that assigns equal probability of selection to every elephant (1/11). With a sample size of 1, there are 11 possible samples. Using the estimator Y=yi/pi, calculate the total weight of the elephants for each sample. Make a histogram of these estimates. Report the mean of these 11 estimates.
```{r}
elephant_df2 <- tibble(
  Elephant = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11),
  Weight = c(9000, 9800, 10000, 10100, 10200, 11000, 11100, 11300, 11700, 12200, 13000)
)%>%
  mutate(Probability=1/11)

weight_estimates2<-elephant_df2%>%
  mutate(estimate=Weight/Probability)

hist(weight_estimates2$estimate)

print(paste0("Mean of 11 estimates: ",mean(weight_estimates2$estimate)))

```

#1c. We have three designs – the manager’s design (take the middle elephant), the statistician’s design (part 1a), and the alternative probability design from part 1b. Describe the strengths and weaknesses of each of the three designs. Which of these designs is the best approach to the problem? Explain.

The strength of the manager's design is that it has the best estimate in the short run. The weakness of the manager's design is that is does not take into account any variation in the weights of the rest of the observations. The strength of the statistician's design is that is has the best estimate of the weight in the long-run. The weakness of the statisticians' design is that there are many times in the short run, its estimate is not close to the weight of the population. The weakness of the alternative probability is that it does not take into account that some elephants are better estimators of the population weight than than other elephants.

#2a. Simulate a posterior distribution using the same approach as outlined on slide 28.
```{r}
M<-2500

results<-as.data.frame(matrix(nrow=M, ncol=3))
names(results)<-c("count","proportion","gte40pct")

theta<-rbeta(M,19,13)

for (i in 1:M){
  results[i,1]<-rbinom(1,470,theta[i])
  results[i,2]<-(results[i,1]+18)/500
  results[i,3]<-ifelse(results[i,2]>=0.4,1,0)
}
```

#2b. Create a histogram like the one on slide 29.
```{r}
hist(results$proportion)
```

#2c. Using these simulated draws, what is the probability that at least 40% of the population has insurance?
```{r}
mean(results$gte40pct)
```

#2d. Using the same population of 500, now change the sample size to 15. We find that 9 have insurance. Simulate a posterior distribution using the same approach as outlines on slide 28.
```{r}
results2<-as.data.frame(matrix(nrow=M, ncol=3))
names(results2)<-c("count","proportion","gte40pct")

theta2<-rbeta(M,10,7)

for (i in 1:M){
  results2[i,1]<-rbinom(1,485,theta[i])
  results2[i,2]<-(results[i,1]+9)/500
  results2[i,3]<-ifelse(results[i,2]>=0.4,1,0)
}
```

#2e. Using these simulated draws, what is the probability that at least 40% of the population has insurance?
```{r}
mean(results2$gte40pct)
```

#2f. How does the smaller sample size in part 2e impact the inference?

The smaller sample size makes us less confident in our inference that at least 40% of the population has insurance, even though both samples have the same rate of having insurance.

#3.We have talked about Bayesian and Frequentist perspectives on statistical inference. Choose one of these perspectives and make a brief defense of it. The length of your argument need not be more than a page. Cover what you see as the major strengths of the perspective.

One major strength of the Bayesian perspective on statistical inference over the frequentist perspective is that Bayestian incorporates prior knowledge. This can be useful in cases where answers to questions may lean on some intuitive knowledge, such as the probability of a coin toss landing on heads being 50%. This property also helps when there are small sample sizes, because we are able to use a baseline of prior knowledge instead of only relying on the data. This creates the posterior, which is something that a frequentist perspective cannot produce.

Another major strength of the Bayesian perspective is that data can be treated as random variables with a probability distribution, while the frequentist perspective treats parameters as fixed but unknown. This allows Bayestian methods to provide a more nuanced understanding of data by being able to attach the data to a probability distribution.

One more major strength of the Bayesian perspective is that it can be updated with new data. A frequentist would need to perform new analysis each time they collected new data, and this new analysis would not take into account the results of any previous analyses. On the other hand, Bayesian methods update their beliefs as new data becomes available. The posterior distribution from one analysis can be used as the prior distribution for the next analysis, and be continuously updated as more data becomes available.