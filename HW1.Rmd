---
title: "HW1"
author: "Jay Westreich"
date: "9/15/23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(stats)
library(markdown)
```

#1. Sampling distribution. Using R or some other software, generate a population of N=10,000 with random draws from a normal distribution with mean=10 and variance=2.
```{r}
population <- rnorm(10000, mean = 10, sd = sqrt(2))
```

#1a. Draw 1 sample of size n=100. Calculate the mean, standard error, and 95% CI.
```{r}
sample_data <- sample(population, 100)
sample_mean <- mean(sample_data)
sample_se <- sd(sample_data) / sqrt(100)
ci_lower <- sample_mean - qt(0.975, df = 99) * sample_se
ci_upper <- sample_mean + qt(0.975, df = 99) * sample_se
list(sample_mean = sample_mean, sample_se = sample_se, ci_lower = ci_lower, ci_upper = ci_upper)
```

#1b. Draw a sample of n=100 from it. Store the mean and variance calculated from this sample. Repeat this process 1,000 times. Although not the full sampling distribution, this should be a good approximation of it. Plot the approximation of the sampling distribution of the mean. Plot the approximation sampling distribution of the variance. Extract the 2.5 and 97.5 percentiles of the sampling distribution of the mean.
```{r}
sampling_means <- vector("numeric", 1000)
sampling_vars <- vector("numeric", 1000)

for (i in 1:1000) {
  sample_data <- sample(population, 100)
  sampling_means[i] <- mean(sample_data)
  sampling_vars[i] <- var(sample_data)
}

sampling_means_df <- tibble(mean = sampling_means)
sampling_vars_df <- tibble(variance = sampling_vars)

ggplot(sampling_means_df, aes(x = mean)) +
  geom_histogram(binwidth = 0.1, fill = "blue", alpha = 0.7) +
  ggtitle("Sampling Distribution of the Mean")

ggplot(sampling_vars_df, aes(x = variance)) +
  geom_histogram(binwidth = 0.1, fill = "red", alpha = 0.7) +
  ggtitle("Sampling Distribution of the Variance")

mean_percentiles <- quantile(sampling_means, c(0.025, 0.975))
list(mean_2.5_percentile = mean_percentiles[1], mean_97.5_percentile = mean_percentiles[2])
```

#1c. How do 1a and 1b compare? Explain the difference between what is being reported in 1a and 1b.

1a estimated the population parameters based on that one sample. 1b approximated the distribution of all possible sample means, providing a more robust understanding of how sample means can vary around the population mean.

#1d. The population in 1a is treated as fixed in a frequentist perspective. Please describe in words how might we formulate this problem from a superpopulation perspective?

In a superpopulation perspective, the population of n=10,000 is considered a random sample drawn from an infinite superpopulation that has its own true mean and variance. We treat this mean and variance as random variables from a distribution. When drawing a sample of size n=100 from your population of 10,000, it is taking a sample from the superpopulation. The mean and 95% confidence interval calculated from this sample are estimates not just for both the sample of 10,000 and the superpopulation.

#2a. If you were to toss a coin, what do you believe is the probability of heads? How would you defend your view?

I believe the probability of heads is 50%. I would defend this view by saying that there are only 2 outcomes, heads or tails, and there is no reason to believe that a fair coin would have one side be more likely than the other.

#2b. Use R to develop an algorithm to simulate a fair coin toss. Toss the coin 10,000 time
```{r}
simulate_coin_toss <- function(n) {
  sample(c("Heads", "Tails"), n, replace = TRUE)
}
results <- simulate_coin_toss(10000)
count_results <- table(results)
count_results
```

#2c. Using the R algorithm from 2b, toss the coin 7 times. What is the estimated probability of heads based on your sample?
```{r}
results_7_tosses <- simulate_coin_toss(7)
count_results_7_tosses <- table(results_7_tosses)
estimated_prob_heads_7 <- count_results_7_tosses["Heads"] / 7
estimated_prob_heads_7
```

#2d. Toss the coin 7001 times. What is the estimated probability of heads basedon your sample?
```{r}
results_7001_tosses <- simulate_coin_toss(7001)
count_results_7001_tosses <- table(results_7001_tosses)
estimated_prob_heads_7001 <- count_results_7001_tosses["Heads"] / 7001
estimated_prob_heads_7001
```

#2e. What do you think is the probability of heads for a fair coin toss? Does the evidence you generated in 2c or 2d change your view?

The probability of heads for a fair coin toss is still 50%. The evidence from 2c and 2d did not change my view. 2c did not have enough trials to get a sample that is representative of the true probability. 2d did have enough trials and was close enough to 50% that it is likely the true probability is 50%.

#3.Consider a population of size N=4 given by X = {4,6,38,72}.
```{r}
pt3_samples <- tibble(
  x1 = c(4, 4, 4, 6, 6, 38),
  x2 = c(6, 38, 72, 38, 72, 72)
)
```

#3a. For all 6 possible simple random samples of size n=2, compute the arithmetic, harmonic, and geometric means.
```{r}
pt3_samples<-pt3_samples%>%
  mutate(arithmetic_mean=(x1+x2)/2)%>%
  mutate(harmonic_mean=(2/((1/x1)+(1/x2))))%>%
  mutate(geometric_mean=sqrt(x1*x2))
pt3_samples
```

#3b. Compute the bias of each estimating procedure.
```{r}
pt3_pop <- c(4, 6, 38, 72)
pop_arithmetic <- mean(pt3_pop)
pop_harmonic <- length(pt3_pop) / sum(1 / pt3_pop)
pop_geometric <- exp(mean(log(pt3_pop)))

pt3_samples<-pt3_samples%>%
  mutate(arithmetic_bias=arithmetic_mean-pop_arithmetic)%>%
  mutate(harmonic_bias=harmonic_mean-pop_harmonic)%>%
  mutate(geometric_bias=geometric_mean-pop_geometric)

bias_summary<-pt3_samples%>%
  summarise(arithmetic_bias=mean(arithmetic_bias),
            harmonic_bias=mean(harmonic_bias),
            geometric_bias=mean(geometric_bias))
bias_summary
```

#3c. Compute the variance of each estimating procedure.
```{r}
pt3_samples<-pt3_samples%>%
  mutate(arithmetic_variance=(arithmetic_mean-mean(arithmetic_mean))^2)%>%
  mutate(harmonic_variance=(harmonic_mean-mean(harmonic_mean))^2)%>%
  mutate(geometric_variance=(geometric_mean-mean(geometric_mean))^2)

variance_summary<-pt3_samples%>%
  summarise(arithmetic_variance=mean(arithmetic_variance),
            harmonic_variance=mean(harmonic_variance),
            geometric_variance=mean(geometric_variance))
variance_summary
```

#3d. Compute the mean square error of each estimating procedure.
```{r}
pt3_samples<-pt3_samples%>%
  mutate(arithmetic_mse=(arithmetic_mean-mean(arithmetic_mean))^2)%>%
  mutate(harmonic_mse=(harmonic_mean-mean(arithmetic_mean))^2)%>%
  mutate(geometric_mse=(geometric_mean-mean(arithmetic_mean))^2)

mse_summary<-pt3_samples%>%
  summarise(arithmetic_mse=mean(arithmetic_mse),
            harmonic_mse=mean(harmonic_mse),
            geometric_mse=mean(geometric_mse))
mse_summary
```

#3e. Based on your results in a)-d), briefly comment on the strengths and weaknesses of each procedure.

While the arithmetic procedure has the least bias and smallest mse, it does have the largest variance. The harmonic procedure seemed to perform the worst, with the largest bias, more variance than the geometric procedure, and the largest mse. The geometric procedure had the smallest variance, but it did not perform well with bias or mse.

#3f. Repeat a)-d) for a different population X = {4,8,10,12}. Briefly discuss how these results update your comments in e).
```{r}
pt3f_samples <- tibble(
  x1 = c(4, 4, 4, 8, 8, 10),
  x2 = c(8, 10, 12, 10, 12, 12)
)

pt3f_samples<-pt3f_samples%>%
  mutate(arithmetic_mean=(x1+x2)/2)%>%
  mutate(harmonic_mean=(2/((1/x1)+(1/x2))))%>%
  mutate(geometric_mean=sqrt(x1*x2))
pt3f_samples

pt3f_pop <- c(4, 8, 10, 12)
pop_arithmetic <- mean(pt3f_pop)
pop_harmonic <- length(pt3f_pop) / sum(1 / pt3f_pop)
pop_geometric <- exp(mean(log(pt3f_pop)))

pt3f_samples<-pt3f_samples%>%
  mutate(arithmetic_bias=arithmetic_mean-pop_arithmetic)%>%
  mutate(harmonic_bias=harmonic_mean-pop_harmonic)%>%
  mutate(geometric_bias=geometric_mean-pop_geometric)

bias_summary<-pt3f_samples%>%
  summarise(arithmetic_bias=mean(arithmetic_bias),
            harmonic_bias=mean(harmonic_bias),
            geometric_bias=mean(geometric_bias))
bias_summary

pt3f_samples<-pt3f_samples%>%
  mutate(arithmetic_variance=(arithmetic_mean-mean(arithmetic_mean))^2)%>%
  mutate(harmonic_variance=(harmonic_mean-mean(harmonic_mean))^2)%>%
  mutate(geometric_variance=(geometric_mean-mean(geometric_mean))^2)

variance_summary<-pt3f_samples%>%
  summarise(arithmetic_variance=mean(arithmetic_variance),
            harmonic_variance=mean(harmonic_variance),
            geometric_variance=mean(geometric_variance))
variance_summary

pt3f_samples<-pt3f_samples%>%
  mutate(arithmetic_mse=(arithmetic_mean-mean(arithmetic_mean))^2)%>%
  mutate(harmonic_mse=(harmonic_mean-mean(arithmetic_mean))^2)%>%
  mutate(geometric_mse=(geometric_mean-mean(arithmetic_mean))^2)

mse_summary<-pt3f_samples%>%
  summarise(arithmetic_mse=mean(arithmetic_mse),
            harmonic_mse=mean(harmonic_mse),
            geometric_mse=mean(geometric_mse))
mse_summary
```
This example makes an even stronger case for using the arithmetic procedure. Here, the arithmetic bias is smallest and the arithmetic mse is smallest, which is what we observed in the first example. In this example, the arithmetic variance also has the smallest variance.

#4. Write a short description of each of the statistical viewpoints -- Frequentist, Bayesian, and Superpopulation. This may be done in a few sentences or paragraph for each viewpoint.

In the Frequentist viewpoint, probabilities are calculated from long-run frequencies of events. This approach relies on the idea that given enough data, the true  parameters, such as mean and variance, of a population can be estimated.
 
The Bayesian viewpoint updates the probability for a hypothesis based on new evidence. This means probability is a measure of belief, and prior beliefs are updated with observed data to produce a posterior distribution.

The Superpopulation viewpoint assumes that the observed sample comes from a larger, hypothetical superpopulation that could theoretically be fully known. This viewpoint relies on studying sample data. One can draw several random samples to account for the variability between different groups in the sample.